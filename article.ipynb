{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/leonseet/reannotated-transformer/blob/main/article.ipynb\" target=\"_blank\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/>\n",
    "</a>\n",
    "<a href=\"https://github.com/leonseet/reannotated-transformer\" target=\"_blank\">\n",
    "  <img src=\"https://img.shields.io/badge/GitHub-Repository-blue?logo=github\" alt=\"View on GitHub\"/>\n",
    "</a>\n",
    "\n",
    "This article is a compilation of the notes I took during my deep dive into understanding the Transformer architecture. I would like to acknowledge [The Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/) for providing such a detailed and insightful guide, complete with code, on the Transformer. Much of the code I wrote, especially the model architecture, references their work.\n",
    "\n",
    "This article is intended for readers who already have a background in neural networks and are interested in understanding how Transformer architecture works under the hood.\n",
    "\n",
    "Throughout my learning journey, I've noticed that there are already many excellent tutorials about Transformer out there. Nonetheless, I hope that my write up here could serve as a companion and offer you another perspective on the subject. Everything you see here is also available as a notebook that you can run in Google Colab [here](https://github.com/leonseet/reannotated-transformer/blob/main/article.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this if running in Google Colab\n",
    "# !pip install pandas==2.2.2\n",
    "# !pip install torch==2.4.0\n",
    "# !pip install altair==4.1\n",
    "# !pip install datasets==2.20.0\n",
    "# !pip install transformers==4.43.3\n",
    "# !pip install nltk==3.9.1\n",
    "# !pip install matplotlib==3.9.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations for the model\n",
    "class configs:\n",
    "    BOS = \"<bos>\"\n",
    "    EOS = \"<eos>\"\n",
    "    PAD = \"<pad>\"\n",
    "    EN_VOCAB_SIZE = 52000  # 1000\n",
    "    DE_VOCAB_SIZE = 52000  # 1000\n",
    "\n",
    "    TRAIN_GPU = 0\n",
    "    NUM_EPOCHS = 10\n",
    "    BASE_LEARNING_RATE = 1.0\n",
    "    BATCH_SIZE = 1\n",
    "    ACCUM_ITER = 10\n",
    "    MAX_PADDING = 72\n",
    "    WARMUP = 3000\n",
    "    FILE_PREFIX = \"multi30k_model_\"\n",
    "    MAX_SEQ_LEN = 5000  # 50\n",
    "    VALIDATION_SAMPLE_SIZE = 100\n",
    "\n",
    "    D_MODEL = 512\n",
    "    N_LAYERS = 6  # 6\n",
    "    N_HEADS = 8  # 8\n",
    "    DROPOUT = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "The Transformer architecture, introduced in the landmark paper *[Attention is All You Need](https://arxiv.org/abs/1706.03762)* in 2017, has played a pivotal role in the development of large language models (LLMs) like ChatGPT, Claude, and Gemini. Originally designed for machine translation, the Transformer model was later adapted for a variety of other tasks, including chat conversations, named entity recognition, text summarization, and more.\n",
    "\n",
    "This article focuses on the original Transformer model used for machine translation, which was developed to address two major limitations of earlier models like RNNs and LSTMs:\n",
    "\n",
    "1. **Limited Memory for Long Sequences**: RNNs and LSTMs struggled to retain information over long sequences due to their sequential nature. This limitation is similar to the \"Telephone\" game, where a message gets increasingly distorted as it is passed along.\n",
    "\n",
    "2. **Inefficient Training Speed**: The sequential processing of words in RNNs and LSTMs meant that each word's processing depended on the previous one. This dependency limited parallel processing, significantly slowing down training and preventing the full utilization of GPU capabilities.\n",
    "\n",
    "The transformative feature of the Transformer architecture is its multi-headed attention mechanism. This mechanism enables the model to understand the context and meaning of each word in a sentence, distinguishing between different uses of the same word, such as \"bank\" in \"river bank\" versus \"commercial bank.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leonseet/Desktop/home/personal/published_articles/reannotated-transformer/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import copy\n",
    "\n",
    "from datasets import load_dataset\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from torch import nn\n",
    "from torch.nn.functional import pad\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "# import configs\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import numpy as np\n",
    "from torch.functional import F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS mode\n"
     ]
    }
   ],
   "source": [
    "def get_device(gpu=None):\n",
    "    \"\"\"\n",
    "    Get the device to be used for training\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"CUDA mode\")\n",
    "        return torch.device(f\"cuda:{gpu}\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        print(\"MPS mode\")\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        print(\"CPU mode\")\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "DEVICE = get_device(configs.TRAIN_GPU)  # Automatically use GPU if available, input GPU device number if you want to use a specific GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Architecture\n",
    "\n",
    "In this article, we'll take a top-down approach to understanding the Transformer architecture, which I believe is the most effective way to grasp the model. The following figures illustrate both the original Transformer architecture and a revised version that we will be implementing. The main difference between them lies in the position of the layer normalization: in the revised architecture, we'll perform pre-layer normalization instead of post-layer normalization. We'll explore this change in more detail later.\n",
    "\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "  <div style=\"margin-right: 10px; text-align: center;\">\n",
    "    <img src=\"images/original_transformer_arch.png\" alt=\"Original Transformer Architecture\" height=\"500\"/>\n",
    "    <p style=\"text-align: center;\"><em>Original Transformer Architecture. <a href=\"https://arxiv.org/pdf/1706.03762\">source</a></em></p>\n",
    "  </div>\n",
    "  <div style=\"text-align: center;\">\n",
    "    <img src=\"images/revised_transformer_arch.png\" alt=\"Revised Transformer Architecure with Pre-layer Norm\" height=\"500\"/>\n",
    "    <p style=\"text-align: center;\"><em>Revised Transformer Architecure with Pre-layer Norm. <a href=\"https://github.com/leonseet/reannotated-transformer/blob/main/images/revised_transformer_arch.png\">source</a></em></p>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "As we examine the entire architecture, you'll notice that it's composed of several key components. Let's now walk through each component in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "\n",
    "The Transformer Block encompasses all the components that make up the entire architecture in the Revised Transformer Model. It includes the embedding layers, encoder layers, decoder layers, and the generator. The generator handles the final linear transformation and softmax operation near the output of the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_emb, tgt_emb, generator):\n",
    "        super().__init__()\n",
    "        self.src_emb = src_emb\n",
    "        self.tgt_emb = tgt_emb\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.generator = generator\n",
    "\n",
    "    def encode(self, x, src_mask):\n",
    "        x = self.src_emb(x)\n",
    "        x = self.encoder(x, src_mask)\n",
    "        return x\n",
    "\n",
    "    def decode(self, enc_out, src_mask, y, tgt_mask):\n",
    "        y = self.tgt_emb(y)\n",
    "        y = self.decoder(y, enc_out, src_mask, tgt_mask)\n",
    "        return y\n",
    "\n",
    "    def forward(self, x, y, src_mask, tgt_mask):\n",
    "        x = self.encode(x, src_mask)\n",
    "        y = self.decode(x, src_mask, y, tgt_mask)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images/arch_generator.png\" alt=\"Generator Blocks\" height=\"500\"/>\n",
    "  <p>\n",
    "    <em>Generator Blocks. <a href=\"https://github.com/leonseet/reannotated-transformer/blob/main/images/arch_generator.png\">source</a></em></p>\n",
    "  </p>\n",
    "</div>\n",
    "\n",
    "Notice that the Generator uses Log Softmax instead of the standard Softmax function in the code snippet below. Log Softmax is simply the logarithm applied after Softmax, but this small change has significant benefits.\n",
    "\n",
    "The Softmax function works by taking the exponential of each element in a vector, which can cause very large values to become even larger and very small values to shrink further. This process can lead to numerical instability, as extreme values can cause the function to produce outputs that are either too large or too small to handle effectively.\n",
    "\n",
    "Log Softmax, on the other hand, transforms the output probabilities into a more balanced range (as shown below) by applying the logarithm, reducing the extremes and resulting in a more stable distribution.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images/log_softmax_vs_softmax.png\" alt=\"Example Image\" width=\"500\"/>\n",
    "  <p>\n",
    "    <em>Comparison of Softmax and Log Softmax. <a href=\"https://www.baeldung.com/cs/softmax-vs-log-softmax#:~:text=When%20calculating%20the%20gradient%2C%20the,training%2C%20especially%20for%20complex%20models\">source</a></em></p>\n",
    "  </p>\n",
    "</div>\n",
    "\n",
    "\n",
    "In backpropagation, the derivative of Log Softmax is also more numerically stable compared to that of Softmax. The Softmax derivative involves division, which can result in NaN (Not a Number) outputs if the denominator becomes unstable or approaches zero. In contrast, the derivative of Log Softmax avoids this issue because it doesn’t involve division, leading to more reliable gradient calculations during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x, logits=False):\n",
    "        x = self.linear(x)\n",
    "        if logits:\n",
    "            return x\n",
    "        else:\n",
    "            return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Normalization\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images/arch_layernorm.png\" alt=\"Layer Normalization Blocks\" height=\"500\"/>\n",
    "  <p>\n",
    "    <em>Layer Normalization Blocks. <a href=\"https://github.com/leonseet/reannotated-transformer/blob/main/images/arch_layernorm.png\">source</a></em></p>\n",
    "  </p>\n",
    "</div>\n",
    "\n",
    "We opted for pre-layer normalization (Pre-LN) instead of post-layer normalization (Post-LN) in our model to stabilize the gradients as the model depth increases. The chart below compares the average gradient magnitudes of weights in the Transformer's FFN sub-layers for three scenarios: Pre-LN, Post-LN, and Post-LN with a learning rate warm-up.\n",
    "\n",
    "With Pre-LN, the gradient magnitudes remain relatively stable across increasing layers, indicating better gradient flow. In contrast, Post-LN shows a rapid increase in gradient magnitude as the layers deepen, which can lead to gradient explosion during training. The Post-LN with warm-up introduces a learning rate scheduling mechanism early in training, which helps maintain small and consistent gradients. This is generally a positive outcome, but it also introduces additional hyperparameters that require careful tuning, making the training process more complex.\n",
    "\n",
    "Although we utilized Pre-LN to avoid the need for warm-up scheduling, we will still utilize warm-up during training later for learning purposes. \n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images/pre_vs_post_layernorm.png\" alt=\"Average Magnitude of Gradients for Weight Matrices, W¹ and W², in the FFN sub-layers\" height=\"200\"/>\n",
    "  <p>\n",
    "    <em>Average Magnitude of Gradients for Weight Matrices, W¹ and W², in the FFN sub-layers. <a href=\"https://arxiv.org/pdf/2002.04745\">source</a></em></p>\n",
    "  </p>\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_mean = x.mean(dim=-1, keepdim=True)\n",
    "        x_std = x.std(dim=-1, keepdim=True)\n",
    "        x = (x - x_mean) / (x_std + self.eps)\n",
    "        x = self.gamma * x + self.beta\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images/arch_encoder.png\" alt=\"Encoder Blocks\" height=\"500\"/>\n",
    "  <p>\n",
    "    <em>Encoder Blocks. <a href=\"https://github.com/leonseet/reannotated-transformer/blob/main/images/arch_encoder.png\">source</a></em></p>\n",
    "  </p>\n",
    "</div>\n",
    "\n",
    "In the context of machine translation, the encoder plays a crucial role in transforming a sentence into a meaningful representation that a model can understand and work with. Initially, the encoder takes in the sentence (which is German in our case) without any inherent understanding of its meaning. However, as the sentence passes through the encoder, it transforms into a rich representation that encapsulates not just the meanings of individual words, but also the complex relationships between them within the context of the entire sentence. This encoded representation can be utilized for a variety of tasks such as classification, text summarization, and more. In our case, this information will be passed to the Decoder's Multi-Head Cross Attention for further processing. We'll dive into the differences between cross-attention and self-attention shortly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, n):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(n)])\n",
    "\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    def __init__(self, d_model, p):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.layernorm = LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        x_ = x.clone()\n",
    "        x = self.layernorm(x)\n",
    "        x = sublayer(x)\n",
    "        x = self.dropout(x)\n",
    "        x += x_\n",
    "        return x\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, mha, ffn, d_model, p):\n",
    "        super().__init__()\n",
    "        self.mha = mha\n",
    "        self.ffn = ffn\n",
    "        self.sublayers = clones(SublayerConnection(d_model, p), 2)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x, enc_self_mask):\n",
    "        x = self.sublayers[0](x, lambda x: self.mha(x, x, x, enc_self_mask))\n",
    "        x = self.sublayers[1](x, lambda x: self.ffn(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layer, n_layers):\n",
    "        super().__init__()\n",
    "        self.layers = clones(layer, n_layers)\n",
    "        self.layernorm = LayerNorm(layer.d_model)\n",
    "\n",
    "    def forward(self, x, enc_self_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_self_mask)\n",
    "        x = self.layernorm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images/arch_decoder.png\" alt=\"Decoder Blocks\" height=\"500\"/>\n",
    "  <p>\n",
    "    <em>Decoder Blocks. <a href=\"https://github.com/leonseet/reannotated-transformer/blob/main/images/arch_decoder.png\">source</a></em></p>\n",
    "  </p>\n",
    "</div>\n",
    "\n",
    "On the other hand, the decoder takes this encoded representation (which is the rich representation from the encoder) and combines it with the translated sentence, which initially consists of a special \\<bos\\> token (beginning of sentence). The decoder then generates words one at a time, producing a probability distribution over possible words from the target language (in our case, English). Depending on the decoding strategy, a word is sampled from this distribution as the next token to be generated. This new token is appended to the target sequence and fed back into the decoder to generate the following word. This iterative process continues until either a maximum sequence length is reached or a special \\<eos\\> token (end of sentence) is generated, signaling the end of the translation.\n",
    "\n",
    "What I have just described for the encoder and decoder is what happens during model inference which differs slightly for model training. In model training, the entire translated sentence is passed into the decoder and not one word at a time. To prevent the model from seeing the entire sentence during training, a mask is added to hide future words from the sentence which is applied in the self-attention mask in the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, self_mha, cross_mha, ffn, d_model, p):\n",
    "        super().__init__()\n",
    "        self.self_mha = self_mha\n",
    "        self.cross_mha = cross_mha\n",
    "        self.ffn = ffn\n",
    "        self.sublayers = clones(SublayerConnection(d_model, p), 3)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, y, x, src_mask, tgt_mask):\n",
    "        y = self.sublayers[0](y, lambda y: self.self_mha(y, y, y, tgt_mask))\n",
    "        y = self.sublayers[1](y, lambda y: self.cross_mha(y, x, x, src_mask))\n",
    "        y = self.sublayers[2](y, lambda y: self.ffn(y))\n",
    "        return y\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layer, n_layers):\n",
    "        super().__init__()\n",
    "        self.layers = clones(layer, n_layers)\n",
    "        self.layernorm = LayerNorm(layer.d_model)\n",
    "\n",
    "    def forward(self, y, x, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            y = layer(y, x, src_mask, tgt_mask)\n",
    "        y = self.layernorm(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Head Attention\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images/arch_mha.png\" alt=\"Multi-Head Attention\" height=\"500\"/>\n",
    "  <p>\n",
    "    <em>Multi-Head Attention Blocks. <a href=\"https://github.com/leonseet/reannotated-transformer/blob/main/images/arch_mha.png\">source</a></em></p>\n",
    "  </p>\n",
    "</div>\n",
    "\n",
    "The attention mechanism, which is the core of the Transformer model, is where the magic happens. After coming across many analogies online, I found [this article by Eugene Yan](https://eugeneyan.com/writing/attention/) to offer one of the best analogy:\n",
    "\n",
    "> Imagine yourself in a library. You have a specific question (query). Books on the shelves have titles on their spines (keys) that suggest their content. You compare your question to these titles to decide how relevant each book is, and how much attention to give each book. Then, you get the information (value) from the relevant books to answer your question.\n",
    "\n",
    "As shown in the architecture diagram above, after the first Pre-LN, the weight matrix is split into three components: query (q), key (k), and value (v). These are all derived from the initial weight matrix but serve different purposes in the attention mechanism. Attention fundamentally relies on two key operations: matrix multiplication and the softmax function. \n",
    "\n",
    "First, the query is compared to the key using a dot product through matrix multiplication, which determines how relevant each key is to the query. The relevance of each key is then quantified using the softmax function, which converts these relevance scores into a probability distribution that sums to 1. Finally, the \"meaning\" of the relevant information is extracted by performing another matrix multiplication with the value vector.\n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images/mha.png\" alt=\"Multi-Head Attention in Detail\" height=\"500\"/>\n",
    "  <p>\n",
    "    <em>Multi-Head Attention in Detail. <a href=\"https://github.com/leonseet/reannotated-transformer/blob/main/images/mha.png\">source</a></em></p>\n",
    "  </p>\n",
    "</div>\n",
    "\n",
    "In the Multi-Head Attention mechanism a shown in the figure above, the term \"multi-head\" refers to splitting the embedding dimension into multiple segments. These segments are processed in parallel, and the results are then stacked back together. The purpose of having multiple heads is to introduce diversity into the representation of each word after attention is applied.\n",
    "\n",
    "For instance, take the sentence \"The quick brown fox jumps over the lazy dog.\" If we focus on the word \"fox,\" different heads might attend to different aspects of the sentence. One head might focus on the appearance of the fox, such as \"brown,\" another might focus on the action, such as \"jumps,\" and a third might focus on the interaction with \"dog.\" By aggregating the outputs from all heads at the end of the MHA, we achieve a more comprehensive and nuanced representation of the word \"fox\" in the embedding vectors. For visual learners, I highly recommend [this video by 3Blue1Brown](https://www.youtube.com/watch?v=eMlx5fFNoYc) to understand the intuition of MHA.\n",
    "\n",
    "As we have alluded previously, there are 2 types of attention mechanism used in the Transformer model, self-attention and cross-attention. \n",
    "\n",
    "Self-attention is used both in the encoder and in the first Multi-Head Attention (MHA) layer of the decoder. In this mechanism, the query, key, and value are derived from the same source, which is the input sentence. The purpose of self-attention is to help the model determine which words within the sentence are most significant and should be focused on. By calculating the relationships between each word in the sentence, the model learns to prioritize certain words over others, depending on their importance in the context of the sentence.\n",
    "\n",
    "Cross-attention, on the other hand, is used in the second MHA layer of the decoder and functions differently. Here, the query and key are not the same. In the context of machine translation, for instance, from German to English, the query is derived from the translated language (English), while the key is derived from the source language (German). The cross-attention mechanism allows the model to create a similarity matrix between the query (English) and the key (German). This matrix helps the model understand which German words should be given more attention for each English word during the translation process.\n",
    "\n",
    "The values (v) that are utilized after applying the softmax function in this mechanism are derived from the key (German). These key and value come from the encoder's output, which means that the model is effectively using information from the original German sentence to inform the translation process into English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_attention(q, k, v, mask, dropout):\n",
    "    d_k = q.shape[-1]\n",
    "    attn = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        attn = attn.masked_fill(mask == 0, -1e9)\n",
    "    attn = F.softmax(attn, -1)\n",
    "    if dropout is not None:\n",
    "        attn = dropout(attn)\n",
    "\n",
    "    value = torch.matmul(attn, v)\n",
    "    return value, attn\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, p=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        self.d_k = d_model // n_heads\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size, max_seq_len, _ = q.shape\n",
    "        q = self.linears[0](q)\n",
    "        k = self.linears[1](k)\n",
    "        v = self.linears[2](v)\n",
    "        q = q.reshape(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        k = k.reshape(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        v = v.reshape(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "\n",
    "        value, attn = scaled_dot_attention(q, k, v, mask, self.dropout)\n",
    "        value = value.transpose(1, 2).reshape(batch_size, max_seq_len, self.n_heads * self.d_k)\n",
    "        value = self.linears[3](value)\n",
    "\n",
    "        del q\n",
    "        del k\n",
    "        del v\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position-wise Feed-Forward Networks\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images/arch_poswiseffn.png\" alt=\"Position-wise FFN\" height=\"500\"/>\n",
    "  <p>\n",
    "    <em>Position-wise FFN Blocks. <a href=\"https://github.com/leonseet/reannotated-transformer/blob/main/images/arch_poswiseffn.png\">source</a></em></p>\n",
    "  </p>\n",
    "</div>\n",
    "\n",
    "This method is called 'Position Wise' because the FFN operates on the embedding dimension of each sequence element individually, rather than on the sequence as a whole. Imagine a sentence where each word is represented by a vector. In this approach, the same FFN is applied to each word's vector independently. Therefore, if the same word appears in multiple positions within the sentence, it will produce the same output after passing through the FFN. Importantly, the vectors of one word do not interact with the vectors of another word in this process. This is why the term 'Position Wise' is used to describe this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFFN(nn.Module):\n",
    "    def __init__(self, d_model, p=0.1):\n",
    "        super().__init__()\n",
    "        self.expand_linear = nn.Linear(d_model, d_model * 4)\n",
    "        self.shrink_linear = nn.Linear(d_model * 4, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.expand_linear(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.shrink_linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Layer & Positional Encoding\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images/arch_emb.png\" alt=\"Embedding Layer & Positional Encoding Blocks\" height=\"500\"/>\n",
    "  <p>\n",
    "    <em>Embedding Layer & Positional Encoding Blocks. <a href=\"https://github.com/leonseet/reannotated-transformer/blob/main/images/arch_emb.png\">source</a></em></p>\n",
    "  </p>\n",
    "</div>\n",
    "\n",
    "\n",
    "In Transformer models, the Embedding Layer plays a crucial role in converting tokens into their corresponding embedding vectors. You can think of this layer as a kind of Python dictionary, where the token IDs are the keys, and the embedding vectors are the values. When you input a list of tokens into the Embedding Layer, it retrieves the embedding vectors for each token from this \"dictionary.\"\n",
    "\n",
    "One important aspect to note is that during the forward pass, the embedding vectors are scaled by the square root of the embedding dimension. This scaling ensures that the values of the embedding vectors are not significantly smaller than the values of the positional encodings (ranges between -1 to 1) that are added to them. Without this scaling, the positional encoding could overpower the embedding vectors, which would disrupt the model's ability to differentiate between the actual token meanings and their positional context.\n",
    "\n",
    "Positional encoding is essential in Transformers because, unlike RNNs, Transformers don't inherently understand the order of tokens. Positional encoding provides the model with information about the position of each token in a sequence, enabling it to understand the relative distances between words. For instance, the word \"She\" five sentences away might refer to a different entity than \"She\" in the current sentence. This positional awareness is crucial for the model's comprehension.\n",
    "\n",
    "There are various strategies to incorporate positional information, such as learned positional encodings or rotary positional encodings. In this article, we'll focus on the sinusoidal positional encoding, as used in the original Transformer paper. The sinusoidal encoding is calculated using the following formulas:\n",
    "\n",
    "$$\n",
    "PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}\\right)\n",
    "$$\n",
    "$$\n",
    "PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}\\right)\n",
    "$$\n",
    "Where:\n",
    "- **i** is the dimension index,\n",
    "- **d_model** is the embedding length,\n",
    "- **pos** is the position of the word in the sentence.\n",
    "\n",
    "The sinusoidal positional encoding has several advantages. First, it can be precomputed, meaning it doesn’t add to the number of learnable parameters in the model. This reduces the overall complexity of the model. Additionally, because the sine and cosine functions are periodic and output values constrained between -1 and 1, they naturally fit well for encoding positional information. Another benefit is that these encodings allow the model to generalize to sequences longer than those seen during training, as the functions smoothly extrapolate beyond the training sequence lengths. This would not have been possible if we were to use learned positional encoding.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x) * math.sqrt(self.d_model)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, max_seq_len, d_model, p=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        pos = torch.arange(0, max_seq_len).unsqueeze(-1)\n",
    "        even_i = torch.arange(0, d_model, 2)\n",
    "        denom = torch.pow(10000, even_i / d_model)\n",
    "        pe[:, 0::2] = torch.sin(pos / denom)\n",
    "        pe[:, 1::2] = torch.cos(pos / denom)\n",
    "        self.register_buffer(\"pe\", pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, : x.shape[1]]\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(src_vocab, tgt_vocab, N=6, d_model=512, h=8, dropout=0.1):\n",
    "    c = copy.deepcopy\n",
    "    position = PositionalEncoding(configs.MAX_SEQ_LEN, d_model, dropout)\n",
    "    src_emb = nn.Sequential(Embeddings(src_vocab, d_model), c(position))\n",
    "    tgt_emb = nn.Sequential(Embeddings(tgt_vocab, d_model), c(position))\n",
    "\n",
    "    mha = MultiHeadAttention(d_model, h, dropout)\n",
    "    pos_ffn = PositionWiseFFN(d_model, dropout)\n",
    "\n",
    "    encoder_layer = EncoderLayer(c(mha), c(pos_ffn), d_model, dropout)\n",
    "    encoder = Encoder(encoder_layer, N)\n",
    "\n",
    "    decoder_layer = DecoderLayer(c(mha), c(mha), c(pos_ffn), d_model, dropout)\n",
    "    decoder = Decoder(decoder_layer, N)\n",
    "\n",
    "    generator = Generator(d_model, tgt_vocab)\n",
    "\n",
    "    model = Transformer(encoder, decoder, src_emb, tgt_emb, generator)\n",
    "\n",
    "    for i, p in enumerate(model.parameters()):\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets Train!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Let's start by training a tokenizer on the [Multi30k Dataset](https://github.com/multi30k/dataset), which contains 29,000 sentences of English-German translations. It's important to note that training a tokenizer is different from training a model. Tokenizer training is a statistical process that uses heuristic rules to create the vocabulary that the model will learn from. Choosing a tokenization strategy (e.g., word-level, character-level, or byte-pair encoding) involves balancing trade-offs between memory usage and processing speed.\n",
    "\n",
    "Character-level encoding reduces the embedding size by representing the entire corpus with a small set of alphanumeric characters (👍 memory). Additionally, it handles out-of-vocabulary (OOV) tokens well since every character in the text is known. However, it requires more tokens for each input, leading to longer sequences that increase the size of the attention layer, which can negate the memory benefits. In addition, the output tokens are longer in sequence which requires more time to generate (👎 speed).\n",
    "\n",
    "In contrast, word-level encoding significantly increases the embedding size (the Oxford English Dictionary estimates around 170,000 words in current use, 👎 memory) but reduces the output sequence length, improving processing speed (👍 speed).\n",
    "\n",
    "To balance these trade-offs, subword tokenization methods like byte-pair encoding (BPE) have become popular. BPE offers a middle ground by reducing memory usage while maintaining reasonable sequence lengths and effectively handling most out-of-vocabulary tokens. For a deeper dive into tokenization, I recommend watching [Andrej Karpathy's video on Tokenization]((https://www.youtube.com/watch?v=zduSFxRajkE))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_corpus(dataset):\n",
    "    return (dataset[start_idx : start_idx + 1000] for start_idx in range(0, len(dataset), 1000))\n",
    "\n",
    "\n",
    "def load_tokenizers():\n",
    "    \"\"\"\n",
    "    Using HuggingFace to load English and German tokenizers from local if they exist,\n",
    "    otherwise build from GPT2's tokenizer using the Multi30k Dataset and save them.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    None\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    src_tokenizer: Tokenizer\n",
    "        Tokenizer for English sentences\n",
    "    tgt_tokenizer: Tokenizer\n",
    "        Tokenizer for German sentences\n",
    "    \"\"\"\n",
    "    if os.path.exists(\"tokenizer/en_tokenizer\") and os.path.exists(\"tokenizer/de_tokenizer\"):\n",
    "        src_tokenizer = AutoTokenizer.from_pretrained(\"tokenizer/en_tokenizer\")\n",
    "        tgt_tokenizer = AutoTokenizer.from_pretrained(\"tokenizer/de_tokenizer\")\n",
    "        print(\"Tokenizers loaded from local.\")\n",
    "    else:\n",
    "        ds = load_dataset(\"bentrevett/multi30k\")\n",
    "        train_dataset = ds[\"train\"]\n",
    "        en_corpus = get_training_corpus(train_dataset[\"en\"])\n",
    "        de_corpus = get_training_corpus(train_dataset[\"de\"])\n",
    "        old_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", bos_token=configs.BOS, eos_token=configs.EOS, pad_token=configs.PAD)\n",
    "        src_tokenizer = old_tokenizer.train_new_from_iterator(en_corpus, vocab_size=configs.EN_VOCAB_SIZE)\n",
    "        tgt_tokenizer = old_tokenizer.train_new_from_iterator(de_corpus, vocab_size=configs.DE_VOCAB_SIZE)\n",
    "        src_tokenizer.save_pretrained(\"tokenizer/en_tokenizer\")\n",
    "        tgt_tokenizer.save_pretrained(\"tokenizer/de_tokenizer\")\n",
    "        print(\"Tokenizers built and saved in tokenizer folder.\")\n",
    "\n",
    "    return src_tokenizer, tgt_tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader\n",
    "Our DataLoader will process and generate data batches on the fly during model training and inference. To optimize performance, we'll tokenize the entire corpus beforehand, as this step can be time-consuming.\n",
    "\n",
    "The DataLoader's key components are the BatchSampler, which determines how to batch the data, and the collate_fn, which generates additional inputs like self and cross-attention masks. We generate masks during batch creation rather than in advance because tokens need padding to the maximum sequence length of each batch before applying masks, and batches are sampled randomly during training, making it impossible to predict mask requirements beforehand.\n",
    "\n",
    "Our BatchSampler (inspired by [An even more annotated Transformer](https://pi-tau.github.io/posts/transformer/)) uses an algorithm to minimize required padding for each batch. It works in the following steps:\n",
    "1. Randomize the entire tokenized dataset.\n",
    "2. Separate the randomized data into pools.\n",
    "3. Sort each pool by descending sequence length.\n",
    "4. Yield batches sequentially from each pool.\n",
    "\n",
    "This approach ensures that batches are similar in length while maintaining randomization, leading to more efficient training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(src_tokenizer, tgt_tokenizer, batch_size=100):\n",
    "    def tokenize_fn(sample):\n",
    "        return tokenize_sample(sample, src_tokenizer, tgt_tokenizer)\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        return collate_batch(batch, src_tokenizer, tgt_tokenizer, DEVICE)\n",
    "\n",
    "    # Load Dataset\n",
    "    ds = load_dataset(\"bentrevett/multi30k\")\n",
    "    train_dataset = ds[\"train\"]\n",
    "    validation_dataset = ds[\"validation\"]\n",
    "    test_dataset = ds[\"test\"]\n",
    "\n",
    "    # Tokenize Dataset\n",
    "    train_tokenized_datasets = train_dataset.map(tokenize_fn, batched=True)\n",
    "    validation_tokenized_datasets = validation_dataset.map(tokenize_fn, batched=True)\n",
    "    test_tokenized_datasets = test_dataset.map(tokenize_fn, batched=True)\n",
    "\n",
    "    train_lengths = [len(src) for src in train_tokenized_datasets[\"src_input_ids\"]]\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_dataloader = DataLoader(\n",
    "        train_tokenized_datasets,\n",
    "        batch_sampler=BatchSampler(train_lengths, batch_size, shuffle=True, drop_last=False),\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "\n",
    "    subset_size = configs.VALIDATION_SAMPLE_SIZE\n",
    "    validation_tokenized_datasets = validation_tokenized_datasets.select(range(subset_size))  # Create a smaller validation set to speed up evaluation during training\n",
    "    validation_dataloader = DataLoader(\n",
    "        validation_tokenized_datasets,\n",
    "        batch_size=subset_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "\n",
    "    test_dataloader = DataLoader(\n",
    "        test_tokenized_datasets,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "\n",
    "    return train_dataloader, validation_dataloader, test_dataloader\n",
    "\n",
    "\n",
    "def tokenize_sample(sample, src_tokenizer, tgt_tokenizer):\n",
    "    src_input_ids = src_tokenizer(sample[\"de\"])[\"input_ids\"]\n",
    "    tgt_input_ids = tgt_tokenizer(sample[\"en\"])[\"input_ids\"]\n",
    "    return {\"src_input_ids\": src_input_ids, \"tgt_input_ids\": tgt_input_ids}\n",
    "\n",
    "\n",
    "def subsequent_mask(max_seq_len):\n",
    "    mask = torch.ones(1, max_seq_len, max_seq_len)\n",
    "    mask = mask.triu(diagonal=0).transpose(-1, -2).type(torch.uint8)\n",
    "    return mask\n",
    "\n",
    "\n",
    "class Batch:\n",
    "    def __init__(self, src, tgt=None, pad=2):  # 2 = <blank>\n",
    "        self.src = src.to(DEVICE)\n",
    "        self.src_mask = (src != pad).unsqueeze(-2).to(DEVICE)\n",
    "        if tgt is not None:\n",
    "            self.tgt = tgt[:, :-1].to(DEVICE)\n",
    "            self.tgt_y = tgt[:, 1:].to(DEVICE)\n",
    "            self.tgt_mask = self.make_std_mask(self.tgt, pad).to(DEVICE)\n",
    "            self.ntokens = (self.tgt_y != pad).data.sum().item()\n",
    "\n",
    "    @staticmethod\n",
    "    def make_std_mask(tgt, pad):\n",
    "        \"Create a mask to hide padding and future words.\"\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "        tgt_mask = tgt_mask & subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data)\n",
    "        return tgt_mask\n",
    "\n",
    "\n",
    "def collate_batch(batch, src_tokenizer, tgt_tokenizer, device):\n",
    "    max_src_length = max([len(sample[\"src_input_ids\"]) for sample in batch])\n",
    "    max_tgt_length = max([len(sample[\"tgt_input_ids\"]) for sample in batch])\n",
    "    tgt_bos_id = torch.tensor([tgt_tokenizer.bos_token_id], device=device).to(torch.int64)\n",
    "    tgt_eos_id = torch.tensor([tgt_tokenizer.eos_token_id], device=device).to(torch.int64)\n",
    "\n",
    "    src_list = []\n",
    "    tgt_list = []\n",
    "\n",
    "    for sample in batch:\n",
    "        src_input_ids = torch.tensor(sample[\"src_input_ids\"], device=device).to(torch.int64)\n",
    "        tgt_input_ids = torch.tensor(sample[\"tgt_input_ids\"], device=device).to(torch.int64)\n",
    "        tgt_input_ids = torch.cat([tgt_bos_id, tgt_input_ids, tgt_eos_id])\n",
    "        src_list.append(pad(src_input_ids, pad=(0, max_src_length - len(src_input_ids)), value=tgt_tokenizer.pad_token_id))\n",
    "        tgt_list.append(pad(tgt_input_ids, pad=(0, max_tgt_length - len(tgt_input_ids)), value=tgt_tokenizer.pad_token_id))\n",
    "\n",
    "    src = torch.stack(src_list)\n",
    "    tgt = torch.stack(tgt_list)\n",
    "    device = f\"cuda:{device}\"\n",
    "    return Batch(src, tgt, pad=src_tokenizer.pad_token_id)\n",
    "\n",
    "\n",
    "class BatchSampler:\n",
    "    def __init__(self, lengths, batch_size, shuffle=True, drop_last=True):\n",
    "        self.lengths = lengths\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.drop_last = drop_last\n",
    "\n",
    "    def __iter__(self):\n",
    "        size = len(self.lengths)\n",
    "        indices = list(range(size))\n",
    "        if self.shuffle:\n",
    "            random.shuffle(indices)\n",
    "\n",
    "        step = min(self.batch_size * 100, size // 4)  # The pool size is 100 times the batch size or 1/4 of the dataset, whichever is smaller\n",
    "\n",
    "        for i in range(0, size, step):\n",
    "            pool = indices[i : i + step]\n",
    "            pool = sorted(pool, key=lambda x: self.lengths[x])\n",
    "            for j in range(0, len(pool), self.batch_size):\n",
    "                batch = pool[j : j + self.batch_size]\n",
    "                if len(batch) < self.batch_size and self.drop_last:\n",
    "                    break\n",
    "                if self.shuffle:\n",
    "                    random.shuffle(batch)\n",
    "                yield batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lengths) // self.batch_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "Following [The Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/), we'll explore the use of KLDivLoss (Kullback-Leibler Divergence Loss) with label smoothing to compute the loss for our Transformer model.\n",
    "\n",
    "**Label smoothing** is a regularization technique applied to hard target labels to prevent model overconfidence during loss computation. It softens target labels and penalizes overly confident outputs. The intuition is to prevent the model from learning that a specific input will always yield a specific output. In Transformer training, instead of assigning 100% probability to a single token, label smoothing slightly lowers its confidence and distributes the remaining probability across other tokens, typically uniformly.\n",
    "\n",
    "Similar to Cross Entropy loss, **KLDivLoss** determines the difference between two data distributions. In Transformers, the output generated for each token is a probability distribution over the vocabulary, with the most likely word having the highest probability. The training loss is the cumulative loss of the predicted distribution against the true distribution of tokens. Unlike Cross Entropy loss, KLDivLoss is typically used when the true distribution is not one-hot encoded. If the true distribution is one-hot encoded, KL Divergence calculation becomes equivalent to Cross Entropy. Since we use label smoothing for the true distribution, KLDivLoss serves as our loss function.\n",
    "\n",
    "More recent decoder architectures or LLMs like GPT and LLaMA use Cross Entropy during pre-training, with one-hot encoded true distributions. Intuitively, one might argue that these models shouldn't be penalized for predictions that are syntactically incorrect but semantically correct. Using Cross Entropy as the loss function with one-hot encoded true distributions might seem to contradict this intuition.\n",
    "\n",
    "However, this potential issue is mitigated in LLMs due to the vast amount of data used for pre-training. The model parameters become generalized over many different one-hot encoded true distributions, allowing the model to learn both syntactic and semantic relationships. While the loss function might appear strict on a per-example basis, the scale and diversity of the training data enable LLMs to capture a broad range of linguistic nuances and meanings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    def __init__(self, vocab_size, padding_idx, smoothing):\n",
    "        super().__init__()\n",
    "        self.padding_idx = padding_idx\n",
    "        self.vocab_size = vocab_size\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.criterion = nn.KLDivLoss(reduction=\"sum\")\n",
    "        self.true_dist = None\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        true_dist = x.data.clone()\n",
    "        true_dist.fill_(self.smoothing / (self.vocab_size - 2))  # 2 because one for target and one for padding\n",
    "        target = target.reshape(-1, 1)\n",
    "        padding = target.data.clone()\n",
    "        padding = padding.fill_(self.padding_idx)\n",
    "        true_dist = true_dist.scatter(-1, target, self.confidence)\n",
    "        true_dist = true_dist.scatter(-1, padding, 0.0)\n",
    "\n",
    "        mask = torch.nonzero(target.data.squeeze(-1) == self.padding_idx)\n",
    "\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(-1), 0.0)\n",
    "\n",
    "        self.true_dist = true_dist\n",
    "        loss = self.criterion(x, true_dist)\n",
    "        return loss\n",
    "\n",
    "\n",
    "class SimpleLossCompute:\n",
    "    \"A simple loss compute and train function.\"\n",
    "\n",
    "    def __init__(self, generator, criterion):\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def __call__(self, x, y, norm):\n",
    "        x = self.generator(x)\n",
    "        sloss = self.criterion(x.contiguous().view(-1, x.size(-1)), y.contiguous().view(-1)) / norm\n",
    "        return sloss.data * norm, sloss\n",
    "\n",
    "\n",
    "def get_loss_fn(model, vocab_size, padding_idx, smoothing):\n",
    "    criterion = LabelSmoothing(vocab_size, padding_idx, smoothing)\n",
    "    criterion.to(DEVICE)\n",
    "    return SimpleLossCompute(model.generator, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "Same as the original paper, we'll be using the Adam optimizer for our model. Adam is a popular choice in deep learning due to its ability to adapt the learning rate for each parameter individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model):\n",
    "    return torch.optim.Adam(model.parameters(), lr=configs.BASE_LEARNING_RATE, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "\n",
    "class DummyOptimizer(torch.optim.Optimizer):\n",
    "    def __init__(self):\n",
    "        self.param_groups = [{\"lr\": 0}]\n",
    "        None\n",
    "\n",
    "    def step(self):\n",
    "        None\n",
    "\n",
    "    def zero_grad(self, set_to_none=False):\n",
    "        None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Scheduler\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images/lr_scheduler.png\" alt=\"Example Image\" width=\"512\"/>\n",
    "  <p>\n",
    "    <em>Change in learning rate over steps at varying model size and warmup rate. <a href=\"https://nlp.seas.harvard.edu/annotated-transformer/\">source</a></em></p>\n",
    "  </p>\n",
    "</div>\n",
    "\n",
    "A warmup learning rate scheduler is used to gradually increase the learning rate to a higher level before slowly decaying it downwards. While there is no theoretical or mathematical proof of why learning rate warmup improves convergence, its effectiveness has been observed empirically. Intuitively, starting from a low learning rate and gradually increasing it allows the model to smoothly transition to adapting to a higher learning rate. This approach helps stabilize training, as starting with an initially high learning rate may cause the training to diverge significantly.\n",
    "\n",
    "Another possible reason for using warmup is to facilitate the calculation of statistics required for adaptive optimizers (e.g., RMSProp, Adam) during the initial few steps. This ensures that the optimizers have optimal statistics when the desired learning rate is reached. ([cite](https://datascience.stackexchange.com/questions/55991/in-the-context-of-deep-learning-what-is-training-warmup-steps)).\n",
    "\n",
    "Once the warmup period reaches the desired learning rate, other schedulers (e.g., exponential decay, cosine annealing) are often applied to continue the training process. These schedulers help fine-tune the learning rate throughout the remainder of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate(step, model_size, factor, warmup):\n",
    "    if step == 0:\n",
    "        step = 1\n",
    "    return factor * (model_size**-0.5 * min(step**-0.5, step * warmup**-1.5))\n",
    "\n",
    "\n",
    "def get_scheduler(optimizer):\n",
    "    return LambdaLR(optimizer=optimizer, lr_lambda=lambda step: rate(step=step, model_size=configs.D_MODEL, factor=1, warmup=configs.WARMUP))\n",
    "\n",
    "\n",
    "class DummyScheduler:\n",
    "    def step(self):\n",
    "        None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding\n",
    "When decoding outputs from LLMs, several common techniques are used, including greedy decoding, beam search, and random sampling, each with its unique characteristics.\n",
    "\n",
    "**Greedy decoding** is the simplest method, where the model consistently selects the most probable token at each step. While efficient, it can lead to suboptimal sequences since it doesn't consider alternatives that might yield a better overall outcome. For simplicity, we'll be using this decoding technique for inference.\n",
    "\n",
    "**Beam search** is designed to produce more probabilistically optimal sequence of outputs by generating multiple candidate sequences and keeping only the most promising ones. This method is particularly effective for tasks requiring high accuracy of the final output sequence generated, such as machine translation. However, beam search is computationally intensive and less flexible, making it less common in modern chat-like LLMs such GPT and Llama. It is also unsuitable for streaming responses as it requires previous tokens to determine next token.\n",
    "\n",
    "**Random sampling** offers more variability and creativity by selecting tokens based on their probability distribution. The degree of randomness can be controlled by parameters like temperature, top-k, and top-p, allowing the output to be either more deterministic or more creative, depending on the needs of the task.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images/temperature.png\" alt=\"alt text\" width=\"500\"/>\n",
    "  <p><em>Probability Distribution of Low vs High Temperature. <a href=\"https://rumn.medium.com/setting-top-k-top-p-and-temperature-in-llms-3da3a8f74832\">source</a></em></p>\n",
    "</div>\n",
    "\n",
    "In random sampling, the **Temperature** parameter, which ranges between 0 and 2, influences the distribution of the model’s log probabilities. It does this by scaling the log probabilities according to the formula `log_prob / temperature`. A lower temperature increases the difference between high and low log probabilities, making the output more deterministic with a peakier probability distribution. Conversely, a higher temperature reduces this contrast, with a smoother probability distribution resulting in more diverse and creative outputs.\n",
    "\n",
    "**Top-k sampling** limits the model’s choices to the top k most probable tokens. For example, if top-k is set to 5, the model will only sample from the top 5 tokens, leading to a more deterministic outcome. When top-k equals 1, this method essentially becomes greedy decoding.\n",
    " \n",
    "**Top-p sampling** (also known as nucleus sampling) works differently by considering the cumulative probability of tokens. For instance, if there are four tokens with probabilities of 0.4, 0.3, 0.2, and 0.1, and top-p is set to 0.8, the model will retain only the first two tokens since their combined probability is 0.7. Including the third token would push the cumulative probability to 0.9, exceeding the top-p threshold.\n",
    " \n",
    "These parameters work together in the following sequence: \n",
    "1. Scale the logits using the temperature factor.\n",
    "2. Sort the scaled logits in descending order.\n",
    "3. Filter out all but the top-k scaled logits.\n",
    "4. Perform softmax on the logits, calculate the cumulative probability at each token and filter out scaled logits above top-p.\n",
    "5. Perform softmax on the filtered scaled logits.\n",
    "6. Sample one token based on this final distribution to obtain the next token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_sampling(model, src, src_mask, max_len, start_idx, end_idx):\n",
    "    ys = torch.tensor(start_idx).to(src.data.dtype).reshape(1, -1).to(DEVICE)\n",
    "    src = src.to(DEVICE)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "\n",
    "    for _ in range(max_len - 1):\n",
    "        x = model.encode(src, src_mask)\n",
    "        logits = model.decode(x, src_mask, ys, subsequent_mask(len(ys)).to(DEVICE))\n",
    "        out = model.generator(logits[:, -1])\n",
    "        _, pred_idx = torch.max(out, dim=-1)\n",
    "        pred_idx = pred_idx.to(src.data.dtype).reshape(1, -1).to(DEVICE)\n",
    "        ys = torch.cat([ys, pred_idx], dim=-1)\n",
    "        if pred_idx == end_idx:\n",
    "            break\n",
    "    return ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have also included an example of random sampling below for reference. In our code, we'll be using greedy sampling, but you can easily switch to the random sampling function to observe the difference. Remember to adjust the hyperparameters such as temperature, top_k, and top_p according to your liking!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sampling(model, src, src_mask, max_len, start_idx, end_idx, temperature=1.5, top_k=10000, top_p=1):\n",
    "    ys = torch.tensor(start_idx).to(src.data.dtype).reshape(1, -1).to(DEVICE)\n",
    "    src = src.to(DEVICE)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "\n",
    "    for _ in range(max_len - 1):\n",
    "        x = model.encode(src, src_mask)\n",
    "        logits = model.decode(x, src_mask, ys, subsequent_mask(len(ys)).to(DEVICE))\n",
    "        logits = model.generator(logits[:, -1], logits=True).reshape(-1)\n",
    "\n",
    "        if temperature != 1.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "        if top_k >= 1:\n",
    "            assert 0 < top_k < len(logits)\n",
    "            top_k_values, _ = torch.topk(logits, top_k)\n",
    "            min_top_k = min(top_k_values)\n",
    "            logits[logits < min_top_k] = -float(\"inf\")\n",
    "\n",
    "        if top_p < 1.0:\n",
    "            sorted_values, sorted_indices = torch.sort(logits, descending=True)\n",
    "            probs = F.softmax(sorted_values, dim=-1)\n",
    "            cumsum_probs = torch.cumsum(probs, dim=-1)\n",
    "            sorted_indices_to_remove = cumsum_probs > top_p\n",
    "            sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()  # Shift to right by 1\n",
    "            sorted_indices_to_remove[0] = 0  # Always keep the first index, so as to prevent error when all indices are above top_p\n",
    "            indices_to_remove = sorted_indices_to_remove.scatter(-1, sorted_indices, sorted_indices_to_remove)\n",
    "            logits[indices_to_remove] = -float(\"inf\")\n",
    "\n",
    "        final_probs = F.softmax(logits, dim=-1)\n",
    "        pred_idx = torch.tensor(torch.multinomial(final_probs, num_samples=1).item())\n",
    "        pred_idx = pred_idx.to(src.data.dtype).reshape(1, -1).to(DEVICE)\n",
    "        ys = torch.cat([ys, pred_idx], dim=-1)\n",
    "        if pred_idx == end_idx:\n",
    "            break\n",
    "    return ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Training Epoch\n",
    "\n",
    "Atlas, we can put everything together to form the training pipeline for one epoch as shown in the code below. We'll be using the Bilingual Evaluation Understudy (BLEU) as our metric to validate the model at the end of each epoch. I'll not be explaining how the BLEU works as the article is long enough. In short, BLEU measures how similar a machine translation is to one or more reference human translations by comparing n-grams. The score ranges from 0 to 1, where 1 indicates a perfect match. You may refer to this [Medium](https://towardsdatascience.com/foundations-of-nlp-explained-bleu-score-and-wer-metrics-1a5ba06d812b) article for more detailed explanation of BLEU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(data_iter, model, loss_compute, optimizer, scheduler, mode=\"train\", accum_iter=1, start_idx=None, end_idx=None, tgt_tokenizer=None, epoch_idx=None):\n",
    "    assert mode in [\"train\", \"eval\"]\n",
    "    start_time = time.time()\n",
    "    total_loss = 0\n",
    "    n_accum = 0\n",
    "    total_tokens = 0\n",
    "    tokens = 0\n",
    "    eos_string = configs.EOS\n",
    "    # pattern = r\"<bos>(.*?)<eos>\"\n",
    "    pattern = r\"(?:<bos>)(.*?)(?:<eos>)\"\n",
    "\n",
    "    predicted_target_tokens = []\n",
    "    gt_target_tokens = []\n",
    "\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        out = model.forward(batch.src, batch.tgt, batch.src_mask, batch.tgt_mask)\n",
    "        loss, loss_node = loss_compute(out, batch.tgt_y, batch.ntokens)\n",
    "\n",
    "        if mode == \"train\":\n",
    "            loss_node.backward()\n",
    "            if i % accum_iter == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                n_accum += 1\n",
    "            scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_tokens += batch.ntokens\n",
    "        tokens += batch.ntokens\n",
    "\n",
    "        if i % 40 == 1 and (mode == \"train\"):\n",
    "            lr = optimizer.param_groups[0][\"lr\"]\n",
    "            elapsed = time.time() - start_time\n",
    "            print(\n",
    "                f\"Epoch Step: {i:6d} | Accumulation Step: {n_accum:3d} | Loss: {loss / batch.ntokens:6.2f} | Efficiency: {tokens / elapsed:7.2f} tok/s | Learning Rate: {lr:6.2e}\"\n",
    "            )\n",
    "            tokens = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "        del loss\n",
    "        del loss_node\n",
    "\n",
    "    if mode == \"eval\":\n",
    "        for i, batch in enumerate(data_iter):\n",
    "            for idx, (src, src_mask, tgt) in enumerate(zip(batch.src, batch.src_mask, batch.tgt)):\n",
    "                print(f\"Validating: {idx +1}/{len(batch.src)}\")\n",
    "                y = greedy_sampling(model, src.unsqueeze(0), src_mask, 72, start_idx, end_idx)\n",
    "                predicted_y = tgt_tokenizer.decode(y[0])\n",
    "                print(\"Predicted: \", predicted_y)\n",
    "                predicted_target_tokens.append(predicted_y)\n",
    "                gt_y = tgt_tokenizer.decode(tgt)\n",
    "                gt_y = gt_y.split(eos_string)[0] + eos_string\n",
    "                print(\"Actual: \", gt_y)\n",
    "                print()\n",
    "                gt_target_tokens.append([gt_y])\n",
    "\n",
    "        gt_target_tokens = [[re.search(pattern, y).group(1) for y in x] for x in gt_target_tokens]\n",
    "        predicted_target_tokens = [x.replace(\"<bos>\", \"\") for x in predicted_target_tokens]\n",
    "        predicted_target_tokens = [x.replace(\"<eos>\", \"\") for x in predicted_target_tokens]\n",
    "        print(\"predicted_target_tokens\", predicted_target_tokens)\n",
    "        bleu_score = corpus_bleu(gt_target_tokens, predicted_target_tokens)\n",
    "        print(f\"BLEU: {bleu_score}\")\n",
    "        # return {\"loss\": total_loss / total_tokens, \"bleu\": bleu_score}\n",
    "\n",
    "    return {\"loss\": total_loss / total_tokens, \"bleu\": bleu_score if mode == \"eval\" else None}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "Our train_worker function helps us orchestrate the model training by:\n",
    "1. Initializing the model architecture.\n",
    "2. Preparing the model for training by setting appropriate modes and configurations.\n",
    "3. Loading all necessary training operators, such as optimizers and loss functions.\n",
    "4. Implementing the main training loop, iterating through epochs to perform model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_worker(src_tokenizer, tgt_tokenizer):\n",
    "    print(f\"Train worker process using GPU: {DEVICE} for training\", flush=True)\n",
    "    start_time = time.time()\n",
    "    pad_idx = src_tokenizer.pad_token_id\n",
    "    start_idx = src_tokenizer.bos_token_id\n",
    "    end_idx = src_tokenizer.eos_token_id\n",
    "    model = make_model(src_tokenizer.vocab_size, tgt_tokenizer.vocab_size, N=configs.N_LAYERS)\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    module = model\n",
    "\n",
    "    train_dataloader, valid_dataloader, _ = create_dataloaders(\n",
    "        src_tokenizer,\n",
    "        tgt_tokenizer,\n",
    "        batch_size=configs.BATCH_SIZE,\n",
    "    )\n",
    "\n",
    "    optimizer = get_optimizer(model)\n",
    "    lr_scheduler = get_scheduler(optimizer)\n",
    "    lose_compute = get_loss_fn(model=model, vocab_size=configs.DE_VOCAB_SIZE, padding_idx=pad_idx, smoothing=0.1)\n",
    "\n",
    "    results = {\"train_loss\": [], \"valid_loss\": [], \"bleu\": []}\n",
    "\n",
    "    for epoch in range(configs.NUM_EPOCHS):\n",
    "        model.train()\n",
    "        print(f\"===> [{DEVICE}] Epoch {epoch} Training\", flush=True)\n",
    "        train_results = run_epoch(\n",
    "            train_dataloader,\n",
    "            model,\n",
    "            lose_compute,\n",
    "            optimizer,\n",
    "            lr_scheduler,\n",
    "            mode=\"train\",\n",
    "            accum_iter=configs.ACCUM_ITER,\n",
    "        )\n",
    "        results[\"train_loss\"].append(train_results[\"loss\"])\n",
    "        os.makedirs(\"models\", exist_ok=True)\n",
    "        if epoch % 5 == 0 or epoch == 0:\n",
    "            file_path = \"models/%s%.2d.pt\" % (configs.FILE_PREFIX, epoch)\n",
    "            torch.save(module.state_dict(), file_path)\n",
    "\n",
    "        print(f\"===> [{DEVICE}] Epoch {epoch} Validation\", flush=True)\n",
    "        model.eval()\n",
    "        valid_results = run_epoch(\n",
    "            valid_dataloader,\n",
    "            model,\n",
    "            lose_compute,\n",
    "            DummyOptimizer(),\n",
    "            DummyScheduler(),\n",
    "            mode=\"eval\",\n",
    "            start_idx=start_idx,\n",
    "            end_idx=end_idx,\n",
    "            tgt_tokenizer=tgt_tokenizer,\n",
    "            epoch_idx=epoch,\n",
    "        )\n",
    "        print(valid_results)\n",
    "        results[\"valid_loss\"].append(valid_results[\"loss\"])\n",
    "        results[\"bleu\"].append(valid_results[\"bleu\"] if \"bleu\" in valid_results else None)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    file_path = \"models/%sfinal.pt\" % configs.FILE_PREFIX\n",
    "    torch.save(module.state_dict(), file_path)\n",
    "    time_elapsed = time.time() - start_time\n",
    "    results[\"total_training_duration\"] = time_elapsed\n",
    "    results[\"epoch\"] = configs.NUM_EPOCHS\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tokenizer, de_tokenizer = load_tokenizers()\n",
    "results = train_worker(src_tokenizer=de_tokenizer, tgt_tokenizer=en_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Does the Model Perform?\n",
    "\n",
    "The training process took approximately 20-30 minutes for 10 epochs on my antique 1080 Ti 👴🏻, which I believe you could do better running this notebook on Google Colab.\n",
    "\n",
    "As we observed from the plots, both the training and validation loss steadily decreased over the epochs, indicating that the model is learning effectively. Additionally, the BLEU score consistently improved from an initial value of 0 to around 0.6, giving us confidence that the model architecture is well-constructed and the training loop is functioning correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "epochs = range(1, len(results[\"train_loss\"]) + 1)\n",
    "ax1.plot(epochs, results[\"train_loss\"], \"b-\", label=\"Train Loss\")\n",
    "ax1.plot(epochs, results[\"valid_loss\"], \"g-\", label=\"Validation Loss\")\n",
    "ax1.set_xlabel(\"Epochs\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.legend()\n",
    "ax1.set_title(\"Train and Validation Loss\")\n",
    "\n",
    "epochs = np.arange(1, len(results[\"bleu\"]) + 1)\n",
    "ax2.plot(epochs, results[\"bleu\"], \"r-\", label=\"Validation BLEU\")\n",
    "ax2.set_xlabel(\"Epochs\")\n",
    "ax2.set_ylabel(\"BLEU Score\")\n",
    "ax2.legend()\n",
    "ax2.set_title(\"BLEU Score\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"training_results.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_outputs(valid_dataloader, model, src_tokenizer, tgt_tokenizer, n_examples, decoding_strategy):\n",
    "    model = model.to(DEVICE).eval()\n",
    "    pad_idx = tgt_tokenizer.pad_token_id\n",
    "    start_idx = tgt_tokenizer.bos_token_id\n",
    "    end_idx = tgt_tokenizer.eos_token_id\n",
    "    eos_string = tgt_tokenizer.eos_token\n",
    "    results = [()] * n_examples\n",
    "\n",
    "    for idx, b in enumerate(valid_dataloader):\n",
    "        if idx == n_examples:\n",
    "            break\n",
    "        print(f\"\\nExample {idx + 1} ========\\n\")\n",
    "        src_tokens = [src_tokenizer.decode(x) for x in b.src[0] if x != pad_idx]\n",
    "        tgt_tokens = [tgt_tokenizer.decode(x) for x in b.tgt_y[0] if x != pad_idx]\n",
    "\n",
    "        print(\"Source Text (Input)        : \" + \" \".join(src_tokens).replace(\"\\n\", \"\"))\n",
    "        print(\"Target Text (Ground Truth) : \" + \" \".join(tgt_tokens).replace(\"\\n\", \"\"))\n",
    "        if decoding_strategy == \"random\":\n",
    "            model_out = random_sampling(model, b.src, b.src_mask, 72, start_idx, end_idx)[0]\n",
    "        else:\n",
    "            model_out = greedy_sampling(model, b.src, b.src_mask, 72, start_idx, end_idx)[0]\n",
    "        model_txt = \" \".join([tgt_tokenizer.decode(x) for x in model_out if x != pad_idx]).split(eos_string, 1)[0] + eos_string\n",
    "        print(\"Model Output               : \" + model_txt.replace(\"\\n\", \"\"))\n",
    "        results[idx] = (b, src_tokens, tgt_tokens, model_out, model_txt)\n",
    "    return results\n",
    "\n",
    "\n",
    "def inference(model_checkpoint, src_tokenizer, tgt_tokenizer, n_examples=5, decoding_strategy=\"greedy\"):\n",
    "    \"\"\"\n",
    "    Inference the model on the test set and print out some examples.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_checkpoint: str\n",
    "        Path to the trained model checkpoint\n",
    "    src_tokenizer: Tokenizer\n",
    "        Tokenizer for source sentences, German in our case\n",
    "    tgt_tokenizer: Tokenizer\n",
    "        Tokenizer for target sentences, English in our case\n",
    "    n_examples: int\n",
    "        Number of examples to print\n",
    "    decoding_strategy: str\n",
    "        Decoding strategy to use, either \"greedy\" or \"random\", remember to tune the temperature, top_k and top_p values if using random sampling\n",
    "    \"\"\"\n",
    "    print(\"Preparing Data ...\")\n",
    "    _, _, test_dataloader = create_dataloaders(\n",
    "        src_tokenizer,\n",
    "        tgt_tokenizer,\n",
    "        batch_size=1,\n",
    "    )\n",
    "\n",
    "    print(\"Loading Trained Model ...\")\n",
    "\n",
    "    model = make_model(src_tokenizer.vocab_size, tgt_tokenizer.vocab_size, N=configs.N_LAYERS)\n",
    "    model.load_state_dict(torch.load(model_checkpoint))\n",
    "\n",
    "    print(\"Checking Model Outputs:\")\n",
    "    example_data = check_outputs(test_dataloader, model, src_tokenizer, tgt_tokenizer, n_examples, decoding_strategy)\n",
    "    return model, example_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing inference on a few examples, we can see that the model is able to generate coherent sentences. However, the performance still seems a little off. Given that the model was trained for only 10 epochs, there's still more room for improvement with additional training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizers loaded from local.\n",
      "Preparing Data ...\n",
      "Loading Trained Model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_210972/2034565918.py:57: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_checkpoint))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Model Outputs:\n",
      "\n",
      "Example 1 ========\n",
      "\n",
      "Source Text (Input)        : Ein  Mann  mit  einem  orangefarbenen  Hut ,  der  etwas  an st arrt .\n",
      "Target Text (Ground Truth) : A  man  in  an  orange  hat  starring  at  something\n",
      "Model Output               : <bos> A  man  wearing  an  orange  hat  is  flipping  a  unique  instrument . <eos>\n",
      "\n",
      "Example 2 ========\n",
      "\n",
      "Source Text (Input)        : Ein  Boston  Terrier  läuft  über  saftig - grünes  Gras  vor  einem  weißen  Zaun .\n",
      "Target Text (Ground Truth) : A  Boston  Terrier  is  running  on  lush  green  grass  in  front  of  a  white  fence\n",
      "Model Output               : <bos> A  athlete  running  through  white  grass  in  front  of  a  white  gate . <eos>\n",
      "\n",
      "Example 3 ========\n",
      "\n",
      "Source Text (Input)        : Ein  Mädchen  in  einem  Karateanzug  bricht  ein  Brett  mit  einem  Tritt .\n",
      "Target Text (Ground Truth) : A  girl  in  karate  uniform  breaking  a  stick  with  a  front  kick\n",
      "Model Output               : <bos> A  girl  in  a  karate  uniform  is  doing  a  board  with  a  sword . <eos>\n",
      "\n",
      "Example 4 ========\n",
      "\n",
      "Source Text (Input)        : Fünf  Leute  in  Winterjacken  und  mit  Helmen  stehen  im  Schnee  mit  Schneemobil en  im  Hintergrund .\n",
      "Target Text (Ground Truth) : Five  people  wearing  winter  jackets  and  helmets  stand  in  the  snow ,  with  snowmobile s  in  the  background\n",
      "Model Output               : <bos> Five  people  in  winter  jackets  and  helmets  stand  in  the  snow  with  palm  trees  in  the  background . <eos>\n",
      "\n",
      "Example 5 ========\n",
      "\n",
      "Source Text (Input)        : Leute  Reparieren  das  Dach  eines  Hauses .\n",
      "Target Text (Ground Truth) : People  are  fixing  the  roof  of  a  house\n",
      "Model Output               : <bos> People  on  the  roof  of  a  house . <eos>\n"
     ]
    }
   ],
   "source": [
    "en_tokenizer, de_tokenizer = load_tokenizers()\n",
    "model, example_data = inference(model_checkpoint=\"models/multi30k_model_final.pt\", src_tokenizer=de_tokenizer, tgt_tokenizer=en_tokenizer, n_examples=5, decoding_strategy=\"greedy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- https://nlp.seas.harvard.edu/annotated-transformer/\n",
    "- https://pi-tau.github.io/posts/transformer/\n",
    "- https://eugeneyan.com/writing/attention/\n",
    "- https://arxiv.org/pdf/1706.03762\n",
    "- https://arxiv.org/pdf/2002.04745"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
